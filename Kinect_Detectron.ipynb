{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries, including Azure Kinect SDK, OpenCV, and Detectron2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Azure Kinect SDK\n",
    "from pykinect_azure.k4a import PyK4A, Config, ImageFormat\n",
    "\n",
    "# Import OpenCV for video processing\n",
    "import cv2\n",
    "\n",
    "# Import Detectron2 for panoptic segmentation\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "\n",
    "# Import additional libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Azure Kinect\n",
    "Set up the Azure Kinect device to start receiving a video stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Azure Kinect\n",
    "# Set up the Azure Kinect device to start receiving a video stream.\n",
    "\n",
    "# Initialize the Azure Kinect device\n",
    "k4a = PyK4A(Config(color_format=ImageFormat.COLOR_BGRA32, color_resolution=720, depth_mode='NFOV_UNBINNED', synchronized_images_only=True))\n",
    "k4a.start()\n",
    "\n",
    "# Function to capture video frames from Azure Kinect\n",
    "def get_video_frame():\n",
    "    capture = k4a.get_capture()\n",
    "    if capture.color is not None:\n",
    "        return capture.color[:, :, :3]  # Return the color image\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Test capturing a frame\n",
    "frame = get_video_frame()\n",
    "if frame is not None:\n",
    "    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGRA2RGB))\n",
    "    plt.title(\"Sample Frame from Azure Kinect\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Failed to capture a frame from Azure Kinect.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Receive Video Stream\n",
    "Capture the video stream from the Azure Kinect device using OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Receive Video Stream\n",
    "# Capture the video stream from the Azure Kinect device using OpenCV.\n",
    "\n",
    "# Function to continuously capture video frames from Azure Kinect\n",
    "def capture_video_stream():\n",
    "    while True:\n",
    "        frame = get_video_frame()\n",
    "        if frame is not None:\n",
    "            # Display the frame\n",
    "            cv2.imshow('Azure Kinect Video Stream', frame)\n",
    "            \n",
    "            # Break the loop if 'q' key is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            print(\"Failed to capture a frame from Azure Kinect.\")\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Start capturing the video stream\n",
    "capture_video_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Panoptic Segmentation Model\n",
    "Load the Detectron2 panoptic segmentation model and configure it to use the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Panoptic Segmentation Model\n",
    "# Load the Detectron2 panoptic segmentation model and configure it to use the GPU.\n",
    "\n",
    "# Set up Detectron2 configuration\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation/panoptic_fpn_R_50_3x.yaml\"))\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation/panoptic_fpn_R_50_3x.yaml\")\n",
    "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Use GPU if available, otherwise CPU\n",
    "\n",
    "# Create the predictor\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# Test the model on a sample frame\n",
    "if frame is not None:\n",
    "    outputs = predictor(frame)\n",
    "    v = Visualizer(frame[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    plt.imshow(out.get_image()[:, :, ::-1])\n",
    "    plt.title(\"Panoptic Segmentation on Sample Frame\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No frame available for testing the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Model on Video Stream\n",
    "Apply the panoptic segmentation model on each frame of the video stream. Adjust the framerate if the model is too slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Model on Video Stream\n",
    "# Apply the panoptic segmentation model on each frame of the video stream. Adjust the framerate if the model is too slow.\n",
    "\n",
    "# Function to apply the panoptic segmentation model on video frames\n",
    "def apply_model_on_video_stream():\n",
    "    while True:\n",
    "        frame = get_video_frame()\n",
    "        if frame is not None:\n",
    "            # Apply the model on the frame\n",
    "            outputs = predictor(frame)\n",
    "            \n",
    "            # Visualize the original frame and the panoptic segmentation result\n",
    "            v = Visualizer(frame[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "            out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "            panoptic_frame = out.get_image()[:, :, ::-1]\n",
    "            \n",
    "            # Concatenate the original frame and the panoptic frame side by side\n",
    "            combined_frame = np.hstack((frame, panoptic_frame))\n",
    "            \n",
    "            # Display the combined frame\n",
    "            cv2.imshow('Original and Panoptic Segmentation', combined_frame)\n",
    "            \n",
    "            # Break the loop if 'q' key is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            print(\"Failed to capture a frame from Azure Kinect.\")\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Start applying the model on the video stream\n",
    "apply_model_on_video_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Original and Segmented Video\n",
    "Create a window to display the original video and the segmented video side by side or top-bottom using OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Original and Segmented Video\n",
    "# Create a window to display the original video and the segmented video side by side or top-bottom using OpenCV.\n",
    "\n",
    "# Function to visualize the original and segmented video frames\n",
    "def visualize_original_and_segmented_video():\n",
    "    while True:\n",
    "        frame = get_video_frame()\n",
    "        if frame is not None:\n",
    "            # Apply the model on the frame\n",
    "            outputs = predictor(frame)\n",
    "            \n",
    "            # Visualize the original frame and the panoptic segmentation result\n",
    "            v = Visualizer(frame[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "            out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "            panoptic_frame = out.get_image()[:, :, ::-1]\n",
    "            \n",
    "            # Concatenate the original frame and the panoptic frame side by side\n",
    "            combined_frame = np.hstack((frame, panoptic_frame))\n",
    "            \n",
    "            # Display the combined frame\n",
    "            cv2.imshow('Original and Panoptic Segmentation', combined_frame)\n",
    "            \n",
    "            # Break the loop if 'q' key is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            print(\"Failed to capture a frame from Azure Kinect.\")\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Start visualizing the original and segmented video\n",
    "visualize_original_and_segmented_video()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
